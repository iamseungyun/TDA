{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gudhi\n",
    "from gudhi.wasserstein import wasserstein_distance\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca291e",
   "metadata": {},
   "source": [
    "## Optimizing point sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b41425",
   "metadata": {},
   "source": [
    "### Rips Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss(pts):\n",
    "    rips = gudhi.RipsComplex(points=pts, max_edge_length=0.5)\n",
    "    # .5 because it is faster and, experimentally, the cycles remain smaller\n",
    "    st = rips.create_simplex_tree(max_dimension=2)\n",
    "    st.compute_persistence()\n",
    "    i = st.flag_persistence_generators()\n",
    "    if len(i[1]) > 0:\n",
    "        i1 = torch.tensor(i[1][0])  # pytorch sometimes interprets it as a tuple otherwise\n",
    "    else:\n",
    "        i1 = torch.empty((0, 4), dtype=int)\n",
    "    # Same as the finite part of st.persistence_intervals_in_dimension(1), but differentiable\n",
    "    diag1 = torch.norm(pts[i1[:, (0, 2)]] - pts[i1[:, (1, 3)]], dim=-1)\n",
    "    # Total persistence is a special case of Wasserstein distance\n",
    "    perstot1 = wasserstein_distance(diag1, [], order=1, enable_autodiff=True)\n",
    "    # Stay within the unit disk\n",
    "    disk = (pts ** 2).sum(-1) - 1\n",
    "    disk = torch.max(disk, torch.zeros_like(disk)).sum()\n",
    "    return -perstot1 + 1 * disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338aacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a square, the loss will round the corners\n",
    "pts = (torch.rand((200, 2)) * 2 - 1).requires_grad_()\n",
    "opt = torch.optim.SGD([pts], lr=1)\n",
    "scheduler = LambdaLR(opt,[lambda epoch: 10./(10+epoch)])\n",
    "for idx in range(600):\n",
    "    opt.zero_grad()\n",
    "    myloss(pts).backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    # Draw every 100 epochs\n",
    "    if idx % 100 == 99:\n",
    "        P = pts.detach().numpy()\n",
    "        plt.scatter(P[:, 0], P[:, 1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a802139",
   "metadata": {},
   "source": [
    "### Hybrid Rips/ alpha complex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d79747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss2(pts):\n",
    "    alpha = gudhi.AlphaComplex(points=pts)\n",
    "    st = alpha.create_simplex_tree()\n",
    "    st.compute_persistence()\n",
    "    p = st.persistence_pairs()\n",
    "    # Keep only pairs that contribute to H1, i.e. (edge, triangle), and separate birth (p1b) and death (p1d)\n",
    "    p1b = torch.tensor([i[0] for i in p if len(i[0]) == 2])\n",
    "    p1d = torch.tensor([i[1] for i in p if len(i[0]) == 2])\n",
    "    if len(p1b) == 0:\n",
    "        # hack\n",
    "        return torch.tensor(0., requires_grad=True)\n",
    "    # Compute the distance between the extremities of the birth edge\n",
    "    b = torch.norm(pts[p1b[:,1]] - pts[p1b[:,0]], dim=-1, keepdim=True)\n",
    "    # For the death triangle, compute the maximum of the pairwise distances\n",
    "    d_1 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
    "    d_2 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,2]], dim=-1, keepdim=True)\n",
    "    d_3 = torch.norm(pts[p1d[:,2]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
    "    d = torch.max(d_1, torch.max(d_2, d_3))\n",
    "    # *Not* the same as the finite part of st.persistence_intervals_in_dimension(1)\n",
    "    diag1 = torch.cat((b,d), 1)\n",
    "    # Total persistence is a special case of Wasserstein distance\n",
    "    perstot1 = wasserstein_distance(diag1, [], order=1, enable_autodiff=True)\n",
    "    # Stay within the unit disk\n",
    "    disk = (pts ** 2).sum(-1) - 1\n",
    "    disk = torch.max(disk, torch.zeros_like(disk)).sum()\n",
    "    return -perstot1 + 1 * disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a square, the loss will round the corners\n",
    "pts = (torch.rand((200, 2)) * 2 - 1).requires_grad_()\n",
    "opt = torch.optim.SGD([pts], lr=1)\n",
    "scheduler = LambdaLR(opt,[lambda epoch: 10./(10+epoch)])\n",
    "for idx in range(600):\n",
    "    opt.zero_grad()\n",
    "    myloss2(pts).backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    if idx % 100 == 99:\n",
    "        P = pts.detach().numpy()\n",
    "        plt.scatter(P[:, 0], P[:, 1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3ca03",
   "metadata": {},
   "source": [
    "### Weak alpha complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e52dfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myloss3(pts):\n",
    "    alpha = gudhi.AlphaComplex(points=pts)\n",
    "    # Build the Delaunay triangulation\n",
    "    st = alpha.create_simplex_tree(default_filtration_value=True)\n",
    "    X = pts.detach().numpy()\n",
    "    for s in st.get_skeleton(1):\n",
    "        if len(s[0]) == 1:\n",
    "            # vertex\n",
    "            st.assign_filtration(s[0], 0)\n",
    "        else:\n",
    "            # edge length\n",
    "            st.assign_filtration(s[0], ((X[s[0][1]] - X[s[0][0]]) ** 2).sum())\n",
    "    # For triangles, use the max of the edges\n",
    "    st.make_filtration_non_decreasing()\n",
    "    st.compute_persistence()\n",
    "    p = st.persistence_pairs()\n",
    "    # Keep only pairs that contribute to H1, i.e. (edge, triangle), and separate birth (p1b) and death (p1d)\n",
    "    p1b = torch.tensor([i[0] for i in p if len(i[0]) == 2])\n",
    "    p1d = torch.tensor([i[1] for i in p if len(i[0]) == 2])\n",
    "    if len(p1b) == 0:\n",
    "        return torch.tensor(0., requires_grad=True)\n",
    "    # Compute the distance between the extremities of the birth edge\n",
    "    b = torch.norm(pts[p1b[:,1]] - pts[p1b[:,0]], dim=-1, keepdim=True)\n",
    "    # For the death triangle, compute the maximum of the pairwise distances\n",
    "    d_1 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
    "    d_2 = torch.norm(pts[p1d[:,1]] - pts[p1d[:,2]], dim=-1, keepdim=True)\n",
    "    d_3 = torch.norm(pts[p1d[:,2]] - pts[p1d[:,0]], dim=-1, keepdim=True)\n",
    "    d = torch.max(d_1, torch.max(d_2, d_3))\n",
    "    # Same as the finite part of st.persistence_intervals_in_dimension(1), but differentiable\n",
    "    diag1 = torch.cat((b,d), 1)\n",
    "    # Total persistence is a special case of Wasserstein distance\n",
    "    perstot1 = wasserstein_distance(diag1, [], order=1, enable_autodiff=True)\n",
    "    # Stay within the unit disk\n",
    "    disk = (pts ** 2).sum(-1) - 1\n",
    "    disk = torch.max(disk, torch.zeros_like(disk)).sum()\n",
    "    return -perstot1 + 1 * disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a059bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a square, the loss will round the corners\n",
    "pts = (torch.rand((200, 2)) * 2 - 1).requires_grad_()\n",
    "opt = torch.optim.SGD([pts], lr=1)\n",
    "scheduler = LambdaLR(opt,[lambda epoch: 10./(10+epoch)])\n",
    "for idx in range(600):\n",
    "    opt.zero_grad()\n",
    "    myloss3(pts).backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    if idx % 100 == 99:\n",
    "        P = pts.detach().numpy()\n",
    "        plt.scatter(P[:, 0], P[:, 1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c394d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
